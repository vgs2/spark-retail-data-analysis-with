{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Using spark to analyse retail data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Dataset used in this example can be found on Kaggle (https://www.kaggle.com/ilkeryildiz/online-retail-listing), if you found it interesting consider upvoting it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the Dataset into a Spark DataFrame"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### We need to load the 'Price' columns as string, because of the decimals being separated by commas. We will adress this in the following blocks."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "import pyspark as spark\r\n",
    "from pyspark.sql.types import StructType\r\n",
    "\r\n",
    "schema = StructType.fromJson({'fields': [{'metadata': {},   'name': 'Invoice',   'nullable': False,   'type': 'integer'},\r\n",
    "{'metadata': {},   'name': 'StockCode',   'nullable': False,   'type': 'string'},\r\n",
    "{'metadata': {},   'name': 'Description',   'nullable': False,   'type': 'string'},\r\n",
    "{'metadata': {},   'name': 'Quantity',   'nullable': False,   'type': 'integer'},\r\n",
    "{'metadata': {},   'name': 'InvoiceDate',   'nullable': False,   'type': 'string'},\r\n",
    "{'metadata': {},   'name': 'Price',   'nullable': False,   'type': 'string'},\r\n",
    "{'metadata': {},   'name': 'Customer_ID',   'nullable': False,   'type': 'integer'},\r\n",
    "{'metadata': {},   'name': 'Country',   'nullable': False,   'type': 'string'}, ],'type': 'struct'})\r\n",
    "df = spark.read.option(\"delimiter\", \";\").schema(schema).format(\"csv\").options(header=\"true\").load(\"online_retail_listing.csv\")\r\n",
    "df.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+---------+--------------------+--------+---------------+-----+-----------+--------------+\n",
      "|Invoice|StockCode|         Description|Quantity|    InvoiceDate|Price|Customer_ID|       Country|\n",
      "+-------+---------+--------------------+--------+---------------+-----+-----------+--------------+\n",
      "| 489434|    85048|15CM CHRISTMAS GL...|      12|1.12.2009 07:45| 6,95|      13085|United Kingdom|\n",
      "| 489434|   79323P|  PINK CHERRY LIGHTS|      12|1.12.2009 07:45| 6,75|      13085|United Kingdom|\n",
      "| 489434|   79323W| WHITE CHERRY LIGHTS|      12|1.12.2009 07:45| 6,75|      13085|United Kingdom|\n",
      "| 489434|    22041|\"RECORD FRAME 7\"\"...|      48|1.12.2009 07:45|  2,1|      13085|United Kingdom|\n",
      "| 489434|    21232|STRAWBERRY CERAMI...|      24|1.12.2009 07:45| 1,25|      13085|United Kingdom|\n",
      "| 489434|    22064|PINK DOUGHNUT TRI...|      24|1.12.2009 07:45| 1,65|      13085|United Kingdom|\n",
      "| 489434|    21871| SAVE THE PLANET MUG|      24|1.12.2009 07:45| 1,25|      13085|United Kingdom|\n",
      "| 489434|    21523|FANCY FONT HOME S...|      10|1.12.2009 07:45| 5,95|      13085|United Kingdom|\n",
      "| 489435|    22350|           CAT BOWL |      12|1.12.2009 07:46| 2,55|      13085|United Kingdom|\n",
      "| 489435|    22349|DOG BOWL , CHASIN...|      12|1.12.2009 07:46| 3,75|      13085|United Kingdom|\n",
      "| 489435|    22195|HEART MEASURING S...|      24|1.12.2009 07:46| 1,65|      13085|United Kingdom|\n",
      "| 489435|    22353|LUNCHBOX WITH CUT...|      12|1.12.2009 07:46| 2,55|      13085|United Kingdom|\n",
      "| 489436|   48173C|DOOR MAT BLACK FL...|      10|1.12.2009 09:06| 5,95|      13078|United Kingdom|\n",
      "| 489436|    21755|LOVE BUILDING BLO...|      18|1.12.2009 09:06| 5,45|      13078|United Kingdom|\n",
      "| 489436|    21754|HOME BUILDING BLO...|       3|1.12.2009 09:06| 5,95|      13078|United Kingdom|\n",
      "| 489436|    84879|ASSORTED COLOUR B...|      16|1.12.2009 09:06| 1,69|      13078|United Kingdom|\n",
      "| 489436|    22119| PEACE WOODEN BLO...|       3|1.12.2009 09:06| 6,95|      13078|United Kingdom|\n",
      "| 489436|    22142|CHRISTMAS CRAFT W...|      12|1.12.2009 09:06| 1,45|      13078|United Kingdom|\n",
      "| 489436|    22296|HEART IVORY TRELL...|      12|1.12.2009 09:06| 1,65|      13078|United Kingdom|\n",
      "| 489436|    22295|HEART FILIGREE DO...|      12|1.12.2009 09:06| 1,65|      13078|United Kingdom|\n",
      "+-------+---------+--------------------+--------+---------------+-----+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Now, let's clean the data\r\n",
    "### Changing Price from string to float and replacing the commas with points"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "from pyspark.sql.functions import regexp_replace\r\n",
    "from pyspark.sql.types import FloatType\r\n",
    "\r\n",
    "df = df.withColumn('Price', regexp_replace('Price', ',', '.'))\r\n",
    "df = df.withColumn('Price', df['Price'].cast(\"float\"))\r\n",
    "df.createOrReplaceTempView(\"retail\")\r\n",
    "df.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+---------+--------------------+--------+---------------+-----+-----------+--------------+\n",
      "|Invoice|StockCode|         Description|Quantity|    InvoiceDate|Price|Customer_ID|       Country|\n",
      "+-------+---------+--------------------+--------+---------------+-----+-----------+--------------+\n",
      "| 489434|    85048|15CM CHRISTMAS GL...|      12|1.12.2009 07:45| 6.95|      13085|United Kingdom|\n",
      "| 489434|   79323P|  PINK CHERRY LIGHTS|      12|1.12.2009 07:45| 6.75|      13085|United Kingdom|\n",
      "| 489434|   79323W| WHITE CHERRY LIGHTS|      12|1.12.2009 07:45| 6.75|      13085|United Kingdom|\n",
      "| 489434|    22041|\"RECORD FRAME 7\"\"...|      48|1.12.2009 07:45|  2.1|      13085|United Kingdom|\n",
      "| 489434|    21232|STRAWBERRY CERAMI...|      24|1.12.2009 07:45| 1.25|      13085|United Kingdom|\n",
      "| 489434|    22064|PINK DOUGHNUT TRI...|      24|1.12.2009 07:45| 1.65|      13085|United Kingdom|\n",
      "| 489434|    21871| SAVE THE PLANET MUG|      24|1.12.2009 07:45| 1.25|      13085|United Kingdom|\n",
      "| 489434|    21523|FANCY FONT HOME S...|      10|1.12.2009 07:45| 5.95|      13085|United Kingdom|\n",
      "| 489435|    22350|           CAT BOWL |      12|1.12.2009 07:46| 2.55|      13085|United Kingdom|\n",
      "| 489435|    22349|DOG BOWL , CHASIN...|      12|1.12.2009 07:46| 3.75|      13085|United Kingdom|\n",
      "| 489435|    22195|HEART MEASURING S...|      24|1.12.2009 07:46| 1.65|      13085|United Kingdom|\n",
      "| 489435|    22353|LUNCHBOX WITH CUT...|      12|1.12.2009 07:46| 2.55|      13085|United Kingdom|\n",
      "| 489436|   48173C|DOOR MAT BLACK FL...|      10|1.12.2009 09:06| 5.95|      13078|United Kingdom|\n",
      "| 489436|    21755|LOVE BUILDING BLO...|      18|1.12.2009 09:06| 5.45|      13078|United Kingdom|\n",
      "| 489436|    21754|HOME BUILDING BLO...|       3|1.12.2009 09:06| 5.95|      13078|United Kingdom|\n",
      "| 489436|    84879|ASSORTED COLOUR B...|      16|1.12.2009 09:06| 1.69|      13078|United Kingdom|\n",
      "| 489436|    22119| PEACE WOODEN BLO...|       3|1.12.2009 09:06| 6.95|      13078|United Kingdom|\n",
      "| 489436|    22142|CHRISTMAS CRAFT W...|      12|1.12.2009 09:06| 1.45|      13078|United Kingdom|\n",
      "| 489436|    22296|HEART IVORY TRELL...|      12|1.12.2009 09:06| 1.65|      13078|United Kingdom|\n",
      "| 489436|    22295|HEART FILIGREE DO...|      12|1.12.2009 09:06| 1.65|      13078|United Kingdom|\n",
      "+-------+---------+--------------------+--------+---------------+-----+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Now it is time to analyse the data\r\n",
    "### Which are the 10 most sold products by total quantity?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "\r\n",
    "spark.sql('SELECT StockCode,  SUM(Quantity) FROM retail GROUP BY StockCode ORDER BY SUM(Quantity) DESC LIMIT 10').show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+-------------+\n",
      "|StockCode|sum(Quantity)|\n",
      "+---------+-------------+\n",
      "|    84077|       107472|\n",
      "|   85123A|        95415|\n",
      "|   85099B|        95290|\n",
      "|    21212|        95136|\n",
      "|    84879|        80007|\n",
      "|    22197|        74828|\n",
      "|    17003|        70423|\n",
      "|    21977|        56366|\n",
      "|    84991|        54271|\n",
      "|    22492|        45164|\n",
      "+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Which are the 10 most sold products by total price spent?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "spark.sql('SELECT StockCode,  SUM(Quantity * Price) AS TotalSpent FROM retail GROUP BY StockCode ORDER BY SUM(Quantity * Price) DESC LIMIT 10').show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+------------------+\n",
      "|StockCode|        TotalSpent|\n",
      "+---------+------------------+\n",
      "|    22423| 322643.1459131241|\n",
      "|      DOT| 304979.9305805862|\n",
      "|   85123A| 251944.6897907257|\n",
      "|   85099B|180281.33941817284|\n",
      "|    47566|147361.55929517746|\n",
      "|    84879|130242.39337158203|\n",
      "|    22086|116439.35060596466|\n",
      "|     POST|110219.20999413729|\n",
      "|    79321| 83145.64911365509|\n",
      "|    22197| 76239.60187804699|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Who are the top 10 purchasers (by total spent) taking only in consideration the 10 most sold products (by quantity)?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "# first the StockCode from the 10 most sold products\r\n",
    "most_sold = 'SELECT StockCode FROM retail GROUP BY StockCode LIMIT 10'\r\n",
    "\r\n",
    "\r\n",
    "spark.sql(f'SELECT Customer_ID,  SUM(Quantity * Price) AS TotalSpent \\\r\n",
    "            FROM retail WHERE StockCode IN ({most_sold}) GROUP BY Customer_ID ORDER BY SUM(Quantity * Price) DESC LIMIT 10').show()\r\n",
    "# spark.sql(f'SELECT * \\\r\n",
    "#             FROM retail WHERE StockCode IN ({most_sold})').show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+------------------+\n",
      "|Customer_ID|        TotalSpent|\n",
      "+-----------+------------------+\n",
      "|       null| 6826.059998869896|\n",
      "|      17450| 2617.199951171875|\n",
      "|      17381|1623.7599487304688|\n",
      "|      14911|1413.5499572753906|\n",
      "|      14646|1045.0199890136719|\n",
      "|      17404|             900.0|\n",
      "|      18109| 713.9999675750732|\n",
      "|      16656| 573.1999778747559|\n",
      "|      13089|505.09997177124023|\n",
      "|      13090| 454.6999797821045|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Which customers bought more products (only regarding quantity)?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "spark.sql(f'SELECT Customer_ID,  SUM(Quantity) AS TotalItemsPurchased \\\r\n",
    "            FROM retail GROUP BY Customer_ID ORDER BY SUM(Quantity) DESC LIMIT 10').show()\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+-------------------+\n",
      "|Customer_ID|TotalItemsPurchased|\n",
      "+-----------+-------------------+\n",
      "|      14646|             357262|\n",
      "|       null|             355706|\n",
      "|      13902|             218090|\n",
      "|      18102|             184412|\n",
      "|      13694|             183002|\n",
      "|      14156|             163910|\n",
      "|      14911|             140184|\n",
      "|      17511|             110698|\n",
      "|      14298|             100176|\n",
      "|      16684|              96363|\n",
      "+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Which customers made more purchases?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "spark.sql(f'SELECT Customer_ID,  COUNT(*) AS TotalPurchases \\\r\n",
    "            FROM retail GROUP BY Customer_ID ORDER BY COUNT(*) DESC LIMIT 10').show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+--------------+\n",
      "|Customer_ID|TotalPurchases|\n",
      "+-----------+--------------+\n",
      "|       null|        236682|\n",
      "|      17841|         12780|\n",
      "|      14911|         11328|\n",
      "|      12748|          7100|\n",
      "|      14606|          6608|\n",
      "|      15311|          4664|\n",
      "|      14096|          4598|\n",
      "|      14156|          4130|\n",
      "|      14646|          3804|\n",
      "|      13089|          3365|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "078700e73cc7da4770e77ddf434ea40c9141e200884cbabb81e2472b976189cc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}